---
title: 'List Models'
description: 'Get available models through the Helicone AI Gateway'
---

## Overview

The models endpoint returns a list of all available models that can be used with the Helicone AI Gateway. This endpoint is OpenAI-compatible and returns models in the same format as OpenAI's `/v1/models` endpoint.

## Authentication

This endpoint does **not** require authentication and can be called without an API key.

## Endpoint

```bash
GET https://ai-gateway.helicone.ai/v1/models
```

## Request

This endpoint accepts no query parameters and returns all available models.

<CodeGroup>
```bash cURL
curl https://ai-gateway.helicone.ai/v1/models
```

```python Python
import requests

response = requests.get(
    "https://ai-gateway.helicone.ai/v1/models"
)

models = response.json()
print(f"Available models: {len(models['data'])}")
```

```javascript JavaScript
fetch('https://ai-gateway.helicone.ai/v1/models')
  .then(response => response.json())
  .then(data => {
    console.log(`Available models: ${data.data.length}`);
    data.data.forEach(model => {
      console.log(`- ${model.id}`);
    });
  });
```

```typescript TypeScript
interface Model {
  id: string;
  object: 'model';
  created: number;
  owned_by: string;
}

interface ModelsResponse {
  object: 'list';
  data: Model[];
}

const response = await fetch('https://ai-gateway.helicone.ai/v1/models');
const models: ModelsResponse = await response.json();

console.log(`Found ${models.data.length} models`);
```
</CodeGroup>

## Response Format

<ResponseField name="object" type="string">
  Always returns `list` to indicate this is a list response.
</ResponseField>

<ResponseField name="data" type="array">
  Array of model objects available through the gateway.
  
  <Expandable title="Model properties">
    <ResponseField name="id" type="string">
      The model identifier. This is the string you use in the `model` parameter when making chat completion requests.
      
      Examples: `gpt-4`, `gpt-3.5-turbo`, `claude-3-opus-20240229`, `gemini-pro`
    </ResponseField>
    
    <ResponseField name="object" type="string">
      Object type, always `model`.
    </ResponseField>
    
    <ResponseField name="created" type="integer">
      Unix timestamp of when the model was created or added to the registry.
    </ResponseField>
    
    <ResponseField name="owned_by" type="string">
      The organization or company that created the model.
      
      Examples: `openai`, `anthropic`, `google`, `meta`, `mistral`
    </ResponseField>
  </Expandable>
</ResponseField>

## Example Response

```json
{
  "object": "list",
  "data": [
    {
      "id": "gpt-4",
      "object": "model",
      "created": 1704067200,
      "owned_by": "openai"
    },
    {
      "id": "gpt-4-turbo",
      "object": "model",
      "created": 1704067200,
      "owned_by": "openai"
    },
    {
      "id": "gpt-3.5-turbo",
      "object": "model",
      "created": 1704067200,
      "owned_by": "openai"
    },
    {
      "id": "claude-3-opus-20240229",
      "object": "model",
      "created": 1704067200,
      "owned_by": "anthropic"
    },
    {
      "id": "claude-3-sonnet-20240229",
      "object": "model",
      "created": 1704067200,
      "owned_by": "anthropic"
    },
    {
      "id": "claude-3-haiku-20240307",
      "object": "model",
      "created": 1704067200,
      "owned_by": "anthropic"
    },
    {
      "id": "gemini-pro",
      "object": "model",
      "created": 1704067200,
      "owned_by": "google"
    },
    {
      "id": "gemini-1.5-pro",
      "object": "model",
      "created": 1704067200,
      "owned_by": "google"
    },
    {
      "id": "llama-3-70b",
      "object": "model",
      "created": 1704067200,
      "owned_by": "meta"
    },
    {
      "id": "mistral-large",
      "object": "model",
      "created": 1704067200,
      "owned_by": "mistral"
    }
  ]
}
```

## Model Filtering

The endpoint automatically filters out models that:
- Require explicit routing configuration
- Are not available through the standard gateway endpoints
- Have no registered endpoints in the cost registry

This ensures you only see models that are immediately usable with the chat completions endpoint.

## Using Model IDs

The `id` field from each model object can be used directly in your chat completion requests:

```json
{
  "model": "gpt-4",
  "messages": [
    {"role": "user", "content": "Hello!"}
  ]
}
```

## Model Support

The Helicone AI Gateway supports models from multiple providers:

- **OpenAI**: GPT-4, GPT-3.5, and variants
- **Anthropic**: Claude 3 family (Opus, Sonnet, Haiku)
- **Google**: Gemini models
- **Meta**: Llama models
- **Mistral**: Mistral and Mixtral models
- **And many more**: The registry is continuously updated with new models

## Error Handling

<ResponseField name="error" type="object">
  Error information if the request fails.
  
  <Expandable title="Error properties">
    <ResponseField name="message" type="string">
      Human-readable error message describing what went wrong.
    </ResponseField>
    
    <ResponseField name="type" type="string">
      Error type, typically `internal_error` for this endpoint.
    </ResponseField>
  </Expandable>
</ResponseField>

### Example Error Response

```json
{
  "error": {
    "message": "Failed to fetch models from registry",
    "type": "internal_error"
  }
}
```

## Implementation Details

The models list is generated from Helicone's internal cost registry (`@helicone-package/cost/models/registry`), which:

- Tracks pricing and capabilities for each model
- Is automatically synced with provider model releases
- Includes metadata like creation dates and ownership
- Powers cost calculation and analytics in Helicone

## Rate Limits

This endpoint has generous rate limits as it's a read-only operation that doesn't consume compute resources. It can be called frequently for model discovery without concerns about rate limiting.

## Best Practices

- **Cache the response**: The model list doesn't change frequently, so cache it client-side to reduce API calls
- **Filter by provider**: If you only use specific providers, filter the response by the `owned_by` field
- **Dynamic model selection**: Use this endpoint to build dynamic model selection UIs
- **Validation**: Validate user-provided model IDs against this list before making completion requests

## Client-Side Filtering Example

```javascript
// Get all OpenAI models
const openaiModels = models.data.filter(m => m.owned_by === 'openai');

// Get all Claude models
const claudeModels = models.data.filter(m => m.owned_by === 'anthropic');

// Check if a model exists
const hasGPT4 = models.data.some(m => m.id === 'gpt-4');

// Get the newest models (sort by created timestamp)
const newestModels = [...models.data]
  .sort((a, b) => b.created - a.created)
  .slice(0, 5);
```

## Related Endpoints

- [Chat Completions](/api/gateway/chat-completions) - Use models from this list to create completions